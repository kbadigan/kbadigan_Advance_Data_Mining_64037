---
title: "project adm"
author: "Karthik Badiganti"
date: "2023-04-19"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading Packages
```{r}
library(caret)
library(dplyr)
library(ISLR)
library(dplyr) 
library(glmnet)
library(plsdepot)
library(randomForest)
library(pROC)
```
## Loading Data

```{r}
data <- read.csv("train_v3.csv")


na_percent <- colSums(is.na(data)) / nrow(data) * 100

max(na_percent)

```

Null values for columns ranges from 0 to 17.82% in each column.

As the variables names have been masked and cannot be interpreted properly.
Near zero variables are removed and highly correlated variables are removed.
Null values are imputed with median impute.

## Removing zero variance and high correlated variables and median imputing

```{r variance removal}
set.seed(0811)
model_filter <- preProcess(data[ ,-c(763)], method = c("nzv", "corr"))
data_filtered <- predict(model_filter, data)
impute_proc <- preProcess(data[ ,-c(763)], method = c("medianImpute"))
data_filtered <- predict(impute_proc , data_filtered)
na_percent_after <- colSums(is.na(data_filtered)) / nrow(data_filtered) * 100
max(na_percent_after)
```

After removing near zero variance and high correlated variables, the variables reduced to 247


## Creating New binary Column for loan default and scaling loss variable
```{r}
data_filtered$loan_default <- as.factor(ifelse(data_filtered$loss>0,"Yes","No"))
data_filtered$loss <- (data_filtered$loss / 100)
```


## Exploring Target Variables
```{r}
percentage <- data_filtered%>%
  group_by(loan_default)  %>%
  summarise(percentage=(n()/nrow(data_filtered))*100, Total=n())
d2<-data_filtered%>%filter(loan_default=='Yes')

p2<-d2%>%group_by(loss)  %>%
  summarise(percentage=(n()/nrow(data_filtered))*100)



ggplot(p2, aes(x=loss, y=percentage)) + 
  geom_bar(stat="identity", position="dodge")  +
  labs(title="Percentage of Loan Loss", 
       x="Loan Loss", y="Percentage", fill="Loss")

# Create a bar chart of the percentages by category
ggplot(percentage, aes(x=loan_default, y=percentage, fill=loan_default)) + 
  geom_bar(stat="identity", position="dodge") + 
  geom_text(aes(label=percentage), position=position_dodge(width=0.9), vjust=-0.5) +
  labs(title="Percentage of Loan Defaults", 
       x="Loan Default", y="Percentage", fill="Loan Default")
```


## Applying lasso to reduce variables
```{r}
set.seed(0811)

lasso_cv <- cv.glmnet( as.matrix(data_filtered[ ,-c(247,248)]),
                       data_filtered$loan,
                       preProcess=c("center","scale"),
                       alpha = 1, family = "binomial", type.measure = "auc")

plot(lasso_cv)
print(paste0("Minimum lambda is ",lasso_cv$lambda.min))
```

## Filtering variables based on Lasso Coefficients
```{r}
set.seed(0811)
lasso_coefs <- coef(lasso_cv, s = "lambda.min")


lasso_coefs_df <- data.frame(name=lasso_coefs@Dimnames[[1]][lasso_coefs@i + 1], coefficient = lasso_coefs@x)

# removing intercept
lasso_coefs_df<- lasso_coefs_df[-1, ]

lasso_names <- as.vector(lasso_coefs_df$name)
lasso_names <- c(lasso_names,"loan_default")
data_filtered_lasso <- select(data_filtered, all_of(lasso_names))
```

After applying lasso model, the variables further reduced to 174


## Applying PCA on the data filtered by lasso

```{r}
preproc_pca<- preProcess(data_filtered_lasso[-c(1,175)], method = c("center", "scale", "pca"), thresh = 0.8)

data_filtered_PCA <- predict(preproc_pca, data_filtered_lasso[-c(1,175)])


data_filtered_PCA$loan_default<-data_filtered_lasso$loan_default

preproc_pca


```



## Creating train data 80% and validation 20%

```{r}
set.seed(0811)

index <- createDataPartition(data_filtered_PCA$loan_default, p = 0.80, list = FALSE)

train_df <- data_filtered_PCA[index, ]
validation_df <-data_filtered_PCA[-index, ]
train_df$loan_default <- as.factor(train_df$loan_default)
validation_df$loan_default <- as.factor(validation_df$loan_default)
```


## Building Random Forest model

```{r}
set.seed(0811)
model_rf <- randomForest(loan_default ~ ., data = train_df,ntre=100,mtry=10)

print(model_rf)
```

## Predicting on Validation Dataset
```{r}
validation_predicted <- data.frame(actual = validation_df$loan_default,
                    predict=predict(model_rf, newdata = validation_df[-68], type = "response"))


confusion_matrix <- confusionMatrix(as.factor(validation_predicted$predict), as.factor(validation_predicted$actual),positive='Yes')

confusion_matrix
```
## Determining threshold cutoff based on AUC
```{r}
library(ROCR)
pred_probs <- predict(model_rf, newdata = validation_df[-68],type='prob')[, "Yes"]

rf.roc <- roc(validation_df$loan_default,pred_probs)

plot(rf.roc)

auc(rf.roc)
```

## Loading Test Data
```{r}
test_data <- read.csv("test__no_lossv3.csv")
```

## preprocessing test data
```{r}
set.seed(0811)
## filtering lasso coefficients
lasso_names_2 <- lasso_names[lasso_names != "loan_default"]
test_data_filtered<-select(test_data, lasso_names_2)

## imputing null values with median impute
final_test_1<- predict(impute_proc, test_data_filtered)

## pca preprocess
final_test_pca<-predict(preproc_pca,final_test_1)



```

## Predicting the Random Forest on test data
```{r}
final_predict<- data.frame(id=test_data$id,
                    predict(model_rf, newdata =final_test_pca[-1], type = "prob"))

final_predict$predict <- ifelse(final_predict$No > 0.59, 0, 1)
test_data$loss<-final_predict$predict
final_test_2<-test_data%>%filter(loss==1)
```



# Building Model for Defaulted Loss Customers and performing Regression


## Creating subset of Defaulters 
```{r subset defaulters}
default_data<- data
default_data$loan_default <- as.factor(ifelse(data_filtered$loss>0,"Yes","No"))
default_data$loss <- (default_data$loss / 100)
default_data <- subset(default_data, default_data$loan_default == 'Yes')
```


## Preprocessing Data
```{r preprocess reg}
set.seed(0811)
model_1 <- preProcess(default_data[ ,-c(763,764)], method = c("nzv", "corr" ))

filtered_default <- predict(model_1, default_data)
impute_proc_2 <- preProcess(filtered_default[ ,-c(763,764)], method = c("medianImpute" ))

filtered_default <- predict(impute_proc_2, filtered_default)
```

This preprocess model took  variables from 762 variables down to 253 variables by removing near zero variables, highly correlated variables, and imputed the median values into missing values.

These values will be fed into our lasso regression model.



##Applying lasso to reduce variables

```{r}

set.seed(0811)

lasso_cv_default <- cv.glmnet(as.matrix(filtered_default[ ,-c(1,252,253)]), filtered_default$loss, alpha = 1, family = "gaussian", type.measure = "mse")
plot(lasso_cv_default)
print(paste0("Optimal Lambda for Lasso regression is ",lasso_cv_default$lambda.min))

```

## Filtering Coefficients based on lasso
```{r}

coefs_default <- coef(lasso_cv_default, s = "lambda.min")

coefs_default_2 <- data.frame(name = coefs_default@Dimnames[[1]][coefs_default@i + 1], coefficient = coefs_default@x)

coefs_default_2 <- coefs_default_2[-1, ]

# Turn the names into a vector

coefs_default_2<- as.vector(coefs_default_2$name)
 
# Add "loss" variable back into the vector

coefs_default_2 <- c(coefs_default_2,"loss")

default_lasso_filtered <- select(filtered_default, all_of(coefs_default_2))

```

Lasso penalized regression further reduced the data set from 253 variables to 115 variables. These will be taken and entered into Ridge regression model to calculate loss given default.


## Partitioning Train 80% and Validation 20%
```{r}

set.seed(0811)

index_2 <- createDataPartition(default_lasso_filtered$loss, p = 0.80, list = FALSE)

train_df_2 <- default_lasso_filtered[index_2, ]
validation_df_2 <- default_lasso_filtered[-index_2, ]

ridge_preproc<-preProcess(train_df_2[-113],method = c("center","scale"))

train_norm<-predict(ridge_preproc,train_df_2)

valid_norm<-predict(ridge_preproc,validation_df_2)
```


## Ridge Regression  for loss given default rate prediction

```{r}

ridge_model <- cv.glmnet(as.matrix(train_norm[ ,-c(113)]),
                         train_norm$loss, 
                         alpha= 0, 
                         family = "gaussian",
                         type.measure = "mae")

plot(ridge_model)

print(paste0("optimal lambda for ridge is ",ridge_model$lambda.min))

coef_ridge <- coef(ridge_model, s = "lambda.min")

```

## Validating Ridge model and calculating MAE
```{r}
predicted_loss <- predict(ridge_model, s = ridge_model$lambda.min, newx =  as.matrix(valid_norm[ ,-c(113)]))


MAE_default = mean(abs((predicted_loss - as.vector(valid_norm$loss))))
comparison <- cbind(as.vector(valid_norm$loss),predicted_loss)

print(paste0("MAE for ridge model is ",MAE_default))
```
## Preprocessing test data
```{r} 

coefs_default_2 <- coefs_default_2[coefs_default_2 != "loss"]


final_test_filtered_2<-select(final_test_2, coefs_default_2)


final_test_filtered_3 <- predict(impute_proc_2, final_test_filtered_2)


final_test_filtered_3 <- predict(ridge_preproc, final_test_filtered_3)
```


## Testing on unknown test data
```{r}

final_test_filtered_4<- as.matrix(final_test_filtered_3)
predicted_loss_test <-  data.frame(id= final_test_2$id,predict=(predict(ridge_model, s = ridge_model$lambda.min, newx = final_test_filtered_4) )*100)

```

## Writing to csv file
```{r}
predicted_loss_test$predicted_loss<-if_else(predicted_loss_test$s1<0,0,round(predicted_loss_test$s1))

pred<-left_join(final_predict,predicted_loss_test,by='id')

pred$loss <- ifelse(pred$predict==0,0,pred$predicted_loss)

final_predicted_file<-data.frame(id=pred$id,loss=pred$loss)

write.csv(final_predicted_file, "final_prediction_file_Group_6.csv", row.names = FALSE)
```




